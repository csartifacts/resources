The purpose of this page is to provide a resource collection about the Artifact Evaluation (AE) process that has been introduced for many publishing venues in Computer Science (CS) in order to promote and help researchers understand (and improve) the AE process.

# General Resources

## Process

* [https://www.artifact-eval.org/](https://www.artifact-eval.org/): General information on the AE process, origins, and packaging guidelines.
* [https://sysartifacts.github.io/](https://sysartifacts.github.io/): Information on the AE processes in system conferences, including calls, committees, and results.

## Advocacy
* [Artefact Review and Badging: Improving Confidence in our Experimental Results](https://acm-fca.org/2018/11/14/artefactreview/) by [Michel Steuwer](https://michel.steuwer.info/)
* [The Real Software Crisis: Repeatability as a Core Value](https://dl.acm.org/doi/10.1145/2658987) by [Shriram Krishnamurthi](https://cs.brown.edu/~sk/) and [Jan Vitek](http://janvitek.org/)

## Artifact Types

* [Proof Artifacts. Guidelines for Submission and Reviewing](https://proofartifacts.github.io/guidelines/) by [Marianna Rapoport](https://mrapoport.com/): Instructions on creating and reviewing proof artifacts.
* [Checking machine-checked proofs](https://project.inria.fr/coqexchange/checking-machine-checked-proofs/) by [Assia Mahboubi](http://people.rennes.inria.fr/Assia.Mahboubi/): Instructions on how to review machine-checked proofs as well as suggestions for authors and organizers.

## Hosting Platforms

* [Zenodo](https://zenodo.org/)
* [Nextjournal](https://nextjournal.com/)
* [Dagstuhl Artifacts Series (DARTS)](https://www.dagstuhl.de/publikationen/darts/)
* [figshare](https://figshare.com/)
* [Dryad](https://datadryad.org/stash)
* [Software Heritage Archive](https://archive.softwareheritage.org/)
* [ACM Digital Library (ACM DL)](https://dl.acm.org/artifacts/software): A collection of artifacts available in the ACM DL.


## Packaging Platforms

* [Docker](https://www.docker.com/)
* [VirtualBox](https://www.virtualbox.org/)
* [Vagrant](https://www.vagrantup.com/)
* [Packer](https://www.packer.io/)

## Experimentation and Evaluation

* [emulab](https://www.emulab.net/): A network testbed to develop, debug, and evaluate systems maintained by the [Flux Research Group](http://www.flux.utah.edu/) at the [University of Utah](https://www.utah.edu/).

## Packaging Instructions

* [Making Your Code Citable](https://guides.github.com/activities/citable-code/): A [GitHub Guide](https://guides.github.com/) with instructions on how to archive [GitHub](https://github.com/) repositories on [Zenodo](https://zenodo.org/).

## Licensing
* [How to license research artifacts?](https://gustavopinto.medium.com/how-to-license-research-artifacts-2ebec048fc87) by [Gustavo Pinto](https://gustavopinto.org/): A general explanation of the purpose of research artifacts and copyright as well as an overview of open-source licenses for various artifact types.
* [Choose an open source license](https://choosealicense.com/): A "keep it simple" guide on choosing open-source licenses.

## Badging

* [Artifact Review and Badging by ACM](https://www.acm.org/publications/policies/artifact-review-and-badging-current): A description of the badges and the requirements for awarding them for ACM conferences.

## Efforts

* [SIGSOFT Artifact Evaluation Working Group](https://github.com/acmsigsoft/artifact-evaluation) by [Jon Bell](https://www.jonbell.net/): A document describing the AE effort in the software engineering field.
* [The ROSE Festival, ESEC/FSE 2018](https://github.com/researchart/rose-fse18/blob/master/cfp.md): The ROSE festival is an effort to promote replication and reproducibility in the software engineering field.

# Artifact Preparation

* [Artifact Evaluation: Tips for Authors](https://blog.padhye.org/Artifact-Evaluation-Tips-for-Authors/) by [Rohan Padhye](https://rohan.padhye.org/): 10 actionable tips for preparing a successful artifact.
* [HOWTO for AEC Submitters](https://docs.google.com/document/d/1pqzPtLVIvwLwJsZwCb2r7yzWMaifudHe1Xvn42T4CcA/edit) by [Dan Barowy](http://www.cs.williams.edu/~dbarowy/), [Charlie Curtsinger](https://curtsinger.cs.grinnell.edu/), [Emma Tosch](https://blog.emmatosch.com/bio.html), and [John Vilk](https://jvilk.com/): Author recommendations for preparing artifacts as well as example artifacts.
* [Creating successful artifacts](https://hernanponcedeleon.github.io/articles/artifacts.html) by [Hernán Ponce de León](https://hernanponcedeleon.github.io/): Tips from an AEC member of POPL, PLDI, and OOPSLA.
* [how to disclose data for double-blind review and make it archived open data upon acceptance](https://ineed.coffee/post/how-to-disclose-data-for-double-blind-review-and-make-it-archived-open-data-upon-acceptance.html) by [Daniel Graziotin](https://ineed.coffee/): Sharing an artifact before paper acceptance in a double-blind way using [figshare](https://figshare.com/) and [Zenodo](https://zenodo.org/).
* [Generating an Artifact From a Benchmarking Setup as Part of CI](https://www.stefan-marr.de/2019/05/artifacts-from-ci/) by [Stefan Marr](https://www.stefan-marr.de/): Automatically generating [VirtualBox](https://www.virtualbox.org/) images as part of the Continuous Integration process.

## Author Experience Reports

* [My first artifact: An author's perspective on the POPL'19 Artifact Evaluation process](http://www.simonjf.com/2018/12/14/popl19-aec.html) by [Simon Fowler](http://www.simonjf.com/): A comprehensive report on an artifact submitted to the POPL AE including an overview of the artifact and its structure, author expectations, the actual reviews as well as reflections on the packaging and the process.
* ["Taming the Parallel Effect Zoo" and the PLDI artifact evaluation process](http://composition.al/blog/2014/03/31/taming-the-parallel-effect-zoo-and-the-pldi-artifact-evaluation-process/) by [Lindsey Kuper](https://users.soe.ucsc.edu/~lkuper/): A report from the author of a PLDI paper including a description of the process, expectations, and artifact packaging.
* [Provisioning My First Artifact.](https://jfdm.github.io/post/2019-04-12-My-First-Artifact.html) by [Jan de Muijnck-Hughes](https://jfdm.github.io/): An experience report by an author of a ECOOP 2019 artifact who packaged his artifact as a [VirtualBox](https://www.virtualbox.org/) image.
* [How Are Award-winning Systems Research Artifacts Prepared](https://www.sigops.org/2021/how-are-award-winning-systems-research-artifacts-prepared-part-1/): An interview of [Manuel Rigger](https://www.manuelrigger.at/) by [Tianyin Xu](https://tianyin.github.io/) for the [ACM SIGOPS blog](https://www.sigops.org/blog/) on the topic of creating successful artifacts.

# Artifact Reviewing

## Reviewer Experience Reports

* [Experience with Artifact Evaluation for CAV 2015](https://dimjasevic.net/marko/2015/05/23/experience-with-artifact-evaluation/index.html) by [Marko Dimjašević](https://dimjasevic.net/marko/): Suggestions on improving the AE process as well as research artifacts based on an experience with the CAV AE process.
* [Why I will never join an Artifacts Evaluation Committee Again](https://inventitech.com/blog/why-i-will-never-review-artifacts-again/) by [Moritz Beller](https://inventitech.com/): Desciption of an (less ideal) experience as a reviewer serving for the ESEC/FSE 2020 AE, including constructive criticism that is likely to serve future AE chairs.
* [Making Sure Artifact Reviewing Stays Anonymous](https://dimjasevic.net/marko/2016/02/17/making-sure-artifact-reviewing-stays-anonymous/index.html) by [Marko Dimjašević](https://dimjasevic.net/marko/): Thoughts and recommendations on maintaining reviewer (and author) anonymity.
* [My Experience of Artifact Evaluation in FPGA2020](https://j7cheng.wordpress.com/2020/07/08/my-experience-of-artifacts-evaluation-in-fpga2020/) by [Jianyi Cheng](https://j7cheng.wordpress.com/): An experience report from the perspective of an author of an artifact as well as an AEC member of FPGA2020, including useful tips for future authors of artifacts.

# AE Chairing

## Chair Reports

* [Reflections on the VMCAI 2021 artifact evaluation](https://sigkill.dk/blog/2020-12-21-vmcai-artifact-evaluation.html) by [Troels Henriksen](https://sigkill.dk/): A short experience report from the perspective of a co-chair of the VMCAI 2021 AE, including suggestions for future editions.
* [OOPSLA 2020 Artifacts Chairs' Report](https://2020.splashcon.org/track/splash-2020-Artifacts#Chairs-Report) by [Colin Gordon](https://www.cs.drexel.edu/~csgordon/) and [Anders Møller](https://www.cs.au.dk/~amoeller/): A report on the OOPSLA 2020 AE featuring recommendations for future artifact evaluations.
* [Artifact Evaluation for Software Conferences](https://cs.brown.edu/~sk/Memos/Conference-Artifact-Evaluation/esec-fse-2011.html) by [Shriram Krishnamurthi](https://cs.brown.edu/~sk/): A report on the first AE process for a major software engineering conference from the perspective of a co-chair.
* [Interview with Eric Bodden and Alessandra Gorla, ISSTA 2016 Artifact Evaluation Chairs](https://issta2016.cispa.saarland/interview-with-eric-bodden-and-alessandra-gorla-issta-2016-artifact-evaluation-chairs/) by [Eric Bodden](https://www.bodden.de/) and [Alessandra Gorla](https://software.imdea.org/~alessandra.gorla/): An interview of the AE co-chairs of ISSTA 2016.

## Detailed Resources for Chairing an AE

* [How to survive an artifact evaluation with HotCRP](https://docs.google.com/document/d/1_Fq4mq5VJs-sMnBs39rTCEDWktb_qhcSeL3d9Kr4cD0/edit) by [Camil Demetrescu](http://www.dis.uniroma1.it/demetres/): Instructions for using HotCRP in the AE process from 2015.
* [ACM PPoPP’19 artifact evaluation report and HotCRP configuration](https://www.linkedin.com/pulse/acm-ppopp19-artifact-evaluation-report-hotcrp-grigori-fursin/) by [Grigori Fursin](http://fursin.net/research.html) and [Flavio Vella](https://www.unibz.it/en/faculties/computer-science/academic-staff/person/39806-flavio-vella): The detailed settings of the HotCRP instance configured for the PPoPP 2019 AE and a short report on the process.
* ["Artifact Evaluation Artifact" for OOPSLA'13](http://evaluate.inf.usi.ch/artifacts/aea) by [Steve Blackburn](https://users.cecs.anu.edu.au/~steveb/) and [Matthias Hauswirth](https://www.inf.usi.ch/faculty/hauswirth/): A detailed description of the first OOPSLA AE process including the timeline, forms, and emails.


# Systematic Studies and Papers

* [Community expectations for research artifacts and evaluation processes](https://dl.acm.org/doi/10.1145/3368089.3409767): A study of AE calls and an analysis of a reviewer survey to study the community expectations in PL/SE conferences
* [Getting Research Software to Work: A Case Study on Artifact Evaluation for OOPSLA 2019](http://lisanqd.com/wp-content/uploads/2019/11/accpub-OOPSLA2019-licensed.pdf) by [Erin Dahlgren](https://edahlgren.github.io/accpub/)
* [Evaluating the artifacts of SIGCOMM papers](https://dl.acm.org/doi/10.1145/3336937.3336944) by Damien Saucez, Luigi Iannone, and Olivier Bonaventure
* [Publish or perish, but do not forget your software artifacts](https://link.springer.com/article/10.1007/s10664-020-09851-6) by [Robert Heumüller](https://cse.cs.ovgu.de/cse/members/robert-heumueller/), [Sebastian Nielebock](https://cse.cs.ovgu.de/cse/members/sebastian-nielebock/), Jacob Krüger, and [Frank Ortmeier](https://cse.cs.ovgu.de/cse/members/frank-ortmeier/)
* [Artifact Evaluation: Is It a Real Incentive?](https://ieeexplore.ieee.org/document/8109184) by [Bruce R. Childers](https://people.cs.pitt.edu/~childers/) and [Panos K. Chrysanthis](https://panos.cs.pitt.edu/)
* [Artifact Evaluation for Publications (Dagstuhl Perspectives Workshop 15452)](https://drops.dagstuhl.de/opus/volltexte/2016/5762/): A report on a Dagstuhl Perspectives Workshop about the AE process.

# Broader Reproducibility and Artifact Resources

* [cTuning](https://ctuning.org/ae/): many useful materials about reproducible science and [AE](https://ctuning.org/ae/).
* [Software Engineering Reproducibility Manifesto (SERM)](https://mboehme.github.io/manifesto) by [Marcel Böhme](https://mboehme.github.io/)
* [Artifact Evaluation for Reproducible Quantitative Research](https://www.sigarch.org/artifact-evaluation-for-reproducible-quantitative-research/)
* [SIGPLAN Empirical Evaluation Checklist](http://www.sigplan.org/Resources/EmpiricalEvaluation/)
